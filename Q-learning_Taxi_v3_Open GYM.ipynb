{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqYW8kb7x6w4lvdAP/TWHi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexTs10/RL_Tinkering/blob/main/Q-learning_Taxi_v3_Open%20GYM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2EBi7aDMMz_e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random \n",
        "\n",
        "## create Taxi env\n",
        "env = gym.make('Taxi-v3')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create a new instance of taxi, and get the initial state\n",
        "state = env.reset()"
      ],
      "metadata": {
        "id": "BfRT1A75NDV9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## random agent script\n",
        "\n",
        "num_steps= 5\n",
        "for s in range(num_steps+1):\n",
        "  print(f\"step: {s} out of {num_steps}\")\n",
        "\n",
        "  # sample a random action from the list of available actions\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  #perform this action on the env.\n",
        "  env.step(action)\n",
        "\n",
        "  #print the new state\n",
        "  env.render()\n",
        "\n",
        "## end this nstance of the taxi env\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT4Rt0wKNYIG",
        "outputId": "ce9918c5-46e4-47a8-8553-d6f4feddb02e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0 out of 5\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|B: |\n",
            "+---------+\n",
            "  (South)\n",
            "step: 1 out of 5\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "step: 2 out of 5\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "step: 3 out of 5\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "step: 4 out of 5\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: |\u001b[43m \u001b[0m: :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "step: 5 out of 5\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: |\u001b[43m \u001b[0m: :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_size = env.observation_space.n  # total number of states (S)\n",
        "action_size = env.action_space.n      # total number of actions (A)\n",
        "\n",
        "## initialize a Qtable with 0's for all Q-values\n",
        "qtable = np.zeros((state_size, action_size))"
      ],
      "metadata": {
        "id": "oDsk7FF6OHXN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters to tune\n",
        "learning_rate = 0.9\n",
        "discount_rate = 0.8\n",
        "\n",
        "# Q learning algorithm: Q(s,a) := Q(s,a) + learning_rate* (reward + discount_rate * max Q(s',a') - Q(s,a))\n",
        "qtable[state, action] += learning_rate* (reward+ discount_rate* np.max(qtable[new_state,:])- qtable[state,action])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "hEVgjX23h-jG",
        "outputId": "c376a6e7-2e66-4412-e10b-67aeaac2fa0e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e2dea8ec6407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Q learning algorithm: Q(s,a) := Q(s,a) + learning_rate* (reward + discount_rate * max Q(s',a') - Q(s,a))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdiscount_rate\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'reward' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exploration-exploitation tradeoff\n",
        "epsilon = 1.0     # probability that our agent will explore\n",
        "decay_rate = 0.01 #of epsilon\n",
        "\n",
        "if random.uniform(0,1) < epsilon:\n",
        "  # explore\n",
        "  action = env.action_space.sample()\n",
        "else:\n",
        "  #exploit\n",
        "  action = np.argmax(qtable[state,:])\n",
        "\n",
        "# epsloon decreases exponentially --> ou agent will explore less and less\n",
        "epsilon = np.exp(-decay_rate*episode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "sfusIrYgmXBv",
        "outputId": "108e05ef-978a-44a6-f194-a18c28e72115"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2e8ab8d2872a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# epsloon decreases exponentially --> ou agent will explore less and less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdecay_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'episode' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### LET'S PUT IT ALL TOGETHER\n",
        "def main():\n",
        "  env = gym.make('Taxi-v3')\n",
        "\n",
        "  state_size = env.observation_space.n  # total number of states (S)\n",
        "  action_size = env.action_space.n      # total number of actions (A)\n",
        "\n",
        "  ## initialize a Qtable with 0's for all Q-values\n",
        "  qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "  # hyperparameters\n",
        "  learning_rate = 0.9\n",
        "  discount_rate = 0.8\n",
        "  epsilon = 1.0\n",
        "  decay_rate= 0.005\n",
        "\n",
        "  # training variables\n",
        "  num_episodes = 1000\n",
        "  max_steps = 99 # per episode\n",
        "\n",
        "  # training\n",
        "  for episode in range(num_episodes):\n",
        "\n",
        "    #reset the environment\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for s in range(max_steps):\n",
        "\n",
        "      # exploration-exploitation tradeoff\n",
        "      if random.uniform(0,1) < epsilon:\n",
        "        # explore\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        # exploit\n",
        "        action = np.argmax(qtable[state,:])\n",
        "    \n",
        "      # take action and observe reward\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "\n",
        "      # Q-learning algorithm\n",
        "      qtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
        "\n",
        "     #update to our new state\n",
        "      state = new_state\n",
        "\n",
        "      #if done, finish episode\n",
        "      if done == True:\n",
        "        break\n",
        "    #decrease epsilon\n",
        "    epsilon = np.exp(-decay_rate* episode)\n",
        "\n",
        "  print(f\"Training completed over {num_episodes} episodes\")\n",
        "  input(\"Press Enter t watch traiing agent...\")\n",
        "\n",
        "  #watch trained agent\n",
        "  state = env.reset()\n",
        "  done= False\n",
        "  rewards = 0\n",
        "\n",
        "  for s in range(max_steps):\n",
        "\n",
        "    print(f\"TRANED AGENT\")\n",
        "    print(\"Step {}\".format(s+1))\n",
        "\n",
        "    action = np.argmax(qtable[state,:])\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    rewards += reward\n",
        "    env.render()\n",
        "    print(f\"score: {rewards}\")\n",
        "    state = new_state\n",
        "    if done== True:\n",
        "      break\n",
        "  \n",
        "  env.close()\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE0o4uzEnY-2",
        "outputId": "206ec48e-80fe-4d0f-f088-7278466b1305"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed over 1000 episodes\n",
            "Press Enter t watch traiing agent...\n",
            "TRANED AGENT\n",
            "Step 1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "score: -1\n",
            "TRANED AGENT\n",
            "Step 2\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "score: -2\n",
            "TRANED AGENT\n",
            "Step 3\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "score: -3\n",
            "TRANED AGENT\n",
            "Step 4\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "score: -4\n",
            "TRANED AGENT\n",
            "Step 5\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "score: -5\n",
            "TRANED AGENT\n",
            "Step 6\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "score: -6\n",
            "TRANED AGENT\n",
            "Step 7\n",
            "+---------+\n",
            "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "score: -7\n",
            "TRANED AGENT\n",
            "Step 8\n",
            "+---------+\n",
            "|\u001b[42mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "score: -8\n",
            "TRANED AGENT\n",
            "Step 9\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "score: -9\n",
            "TRANED AGENT\n",
            "Step 10\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "score: -10\n",
            "TRANED AGENT\n",
            "Step 11\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "score: -11\n",
            "TRANED AGENT\n",
            "Step 12\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "score: -12\n",
            "TRANED AGENT\n",
            "Step 13\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "score: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vu2QiMT9qi8n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}